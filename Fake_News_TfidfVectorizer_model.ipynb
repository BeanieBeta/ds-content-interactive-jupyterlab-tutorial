{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake News TfidfVectorizer model",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeanieBeta/ds-content-interactive-jupyterlab-tutorial/blob/master/Fake_News_TfidfVectorizer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC8ZLxXE94vu"
      },
      "source": [
        "# importing relevant libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuXXl07n-NkJ",
        "outputId": "e9b62f5b-e949-4337-8ecb-8f30de8701a2"
      },
      "source": [
        "# read the datasets from Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "df_fake=pd.read_csv('/content/gdrive/My Drive/Fake.csv')\n",
        "df_true=pd.read_csv('/content/gdrive/My Drive/True.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ_dlvDG-jDm"
      },
      "source": [
        "# add a 'label' column filled with the booleans False and True for the fake news and true news datasets respectively\n",
        "df_fake['label'] = False\n",
        "df_true['label'] = True\n",
        "# combines and randomizes the two datasets\n",
        "combine_set = pd.concat([df_fake,df_true]).sample(frac =  1,random_state = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qgFBnExuPNF"
      },
      "source": [
        "# Gets the labels from the label column\n",
        "labels = combine_set.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f04vPaNpu0Js",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce5d47ae-f3a5-4a28-a3cf-0b199723f9b2"
      },
      "source": [
        "# equally splits the combined dataset into test and training datasets; \n",
        "# separates text and label columns from the combined data sets\n",
        "# uses 80% of each divided part of the dataset as a training set \n",
        "# the other 20%s become test sets \n",
        "# randomizes divided parts of dataset 7 times\n",
        "x_train,x_test,y_train,y_test=train_test_split(combine_set['text'], labels, test_size=0.2, random_state=7)\n",
        "cv=CountVectorizer(max_df=0.85,stop_words='english',max_features=10000)\n",
        "word_count_vector=cv.fit_transform(x_test)\n",
        "list(cv.vocabulary_.keys())[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['time',\n",
              " 'cracking',\n",
              " 'jokes',\n",
              " 'kids',\n",
              " 'middle',\n",
              " 'address',\n",
              " 'world',\n",
              " 'horrible',\n",
              " 'terrorist',\n",
              " 'attack']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frEgLygzB-Lu"
      },
      "source": [
        "# this basically starts a feature extraction command that turns text documents into TF-IDF features\n",
        "# TF-IDF features are bascially words that have been given a numerical value to represent how significant they are\n",
        "# stop_words='english' removes all english stop words from the text i.e. \"a\", \"an\", \"the\", \"be\", etc.\n",
        "# max_df = 0.7 makes it so that all words that appear in more than 70% of the given documents are not included when making the features\n",
        "# it filters these because at a certain point of commonality, certain words just become noise for the model like stop words; they don't help distinguish False articles from True articles \n",
        "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "# Basically applies the above filters to the test and training sets for one category of label\n",
        "tfidf_train=tfidf_vectorizer.fit_transform(x_train) \n",
        "tfidf_test=tfidf_vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHXHzP0DCRwn",
        "outputId": "e2f15d8f-43d3-4342-f4cf-76e1cc96ad4e"
      },
      "source": [
        "# initializes a Passive Agressive Classifier machine learning algorithm and assigns it to \"pac\"; makes it so that this model will go over training data 50 times; \n",
        "# the more times a model runs on a training set, the more accurate it is, but it is important not to overfit the model, so 50 is a reasonable number of times to run it through the training set\n",
        "# passive-aggressive algorithms are a family of machine learning algorithms for large-scale learning(perfect for our datasets with 20,000+ articles)\n",
        "pac=PassiveAggressiveClassifier(max_iter=50)\n",
        "pac.fit(tfidf_train,y_train)\n",
        "# predict on the test set and calculate accuracy, recall, precision, and f1\n",
        "y_pred=pac.predict(tfidf_test)\n",
        "print(f'Accuracy: {round(accuracy_score(y_test,y_pred)*100,2)}%')\n",
        "print(f'Recall: {round(recall_score(y_test,y_pred)*100,2)}%')\n",
        "print(f'Precision: {round(precision_score(y_test,y_pred)*100,2)}%')\n",
        "print(f'F1: {round(f1_score(y_test,y_pred)*100,2)}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 99.4%\n",
            "Recall: 99.58%\n",
            "Precision: 99.16%\n",
            "F1: 99.37%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prd58MOfQOf-"
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    \n",
        "    # word index and corresponding tf-idf score\n",
        "    for idx, score in sorted_items:\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-_rRozfLlpO",
        "outputId": "1e1c2bc5-50b7-4dcb-cfe5-5ee675ed3ce7"
      },
      "source": [
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)\n",
        "feature_names=cv.get_feature_names()\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform(x_test))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "#extract only the top n; n here is 10\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,100)\n",
        "print(\"\\n===Keywords===\")\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===Keywords===\n",
            "watch 1.0\n",
            "spread 1.0\n",
            "losers 1.0\n",
            "great 1.0\n",
            "divide 1.0\n",
            "appear 1.0\n",
            "popick 0.885\n",
            "fundraiser 0.878\n",
            "guo 0.861\n",
            "barrett 0.856\n",
            "watters 0.852\n",
            "00pm 0.85\n",
            "penn 0.847\n",
            "youtu 0.846\n",
            "bannon 0.838\n",
            "electors 0.836\n",
            "update 0.835\n",
            "ailes 0.771\n",
            "veterans 0.824\n",
            "race 0.823\n",
            "memes 0.821\n",
            "ramaphosa 0.82\n",
            "hannity 0.817\n",
            "bharara 0.817\n",
            "tapper 0.815\n",
            "band 0.815\n",
            "tyson 0.813\n",
            "progressives 0.812\n",
            "edt 0.797\n",
            "shorter 0.807\n",
            "words 0.807\n",
            "senecal 0.803\n",
            "menendez 0.802\n",
            "miss 0.801\n",
            "burns 0.8\n",
            "dolly 0.799\n",
            "riina 0.797\n",
            "flake 0.793\n",
            "macy 0.775\n",
            "dayton 0.792\n",
            "keurig 0.792\n",
            "shumpert 0.792\n",
            "franken 0.792\n",
            "taiwan 0.789\n",
            "clyburn 0.787\n",
            "conyers 0.787\n",
            "rohrabacher 0.787\n",
            "pakistan 0.786\n",
            "limit 0.785\n",
            "saakashvili 0.785\n",
            "jackson 0.784\n",
            "mccain 0.783\n",
            "predictions 0.782\n",
            "kerry 0.78\n",
            "cher 0.779\n",
            "stewart 0.779\n",
            "johnston 0.778\n",
            "outrageous 0.778\n",
            "cuomo 0.778\n",
            "grayson 0.777\n",
            "moore 0.777\n",
            "garland 0.776\n",
            "boehner 0.775\n",
            "sessions 0.772\n",
            "kosovo 0.772\n",
            "soros 0.769\n",
            "sabo 0.772\n",
            "duterte 0.772\n",
            "lance 0.771\n",
            "nye 0.771\n",
            "sarsour 0.77\n",
            "brotherhood 0.77\n",
            "gorka 0.769\n",
            "cruz 0.768\n",
            "becky 0.767\n",
            "warmbier 0.766\n",
            "gore 0.765\n",
            "awkward 0.765\n",
            "saleh 0.761\n",
            "ginsburg 0.764\n",
            "darkness 0.763\n",
            "branson 0.762\n",
            "murphy 0.758\n",
            "source 0.757\n",
            "barber 0.757\n",
            "koch 0.757\n",
            "trek 0.755\n",
            "equifax 0.755\n",
            "blasio 0.755\n",
            "socialist 0.753\n",
            "jalal 0.752\n",
            "flynn 0.751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEd_l0SUlhH8",
        "outputId": "688670f1-b031-4381-da68-28b8a214e740"
      },
      "source": [
        "print(sorted_items)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}